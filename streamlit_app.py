import streamlit as st
from veriexcite import (
    extract_bibliography_section,
    split_references,
    search_title,
    set_google_api_key,
    ReferenceStatus,  # new import
)
import io
import pandas as pd
import PyPDF2


def extract_text_from_pdf(pdf_file: st.runtime.uploaded_file_manager.UploadedFile) -> str:
    """Validates if the file is a PDF, then extract text."""
    if not pdf_file.name.lower().endswith(".pdf"):
        raise ValueError("Uploaded file is not a PDF.")
    pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file.read()))
    pdf_content = ""
    for page in pdf_reader.pages:
        page_text = page.extract_text()
        if page_text:
            pdf_content += page_text + "\n"
    return pdf_content


def process_and_verify(bib_text: str, keywords=["Reference", "Bibliography", "Works Cited"]) -> pd.DataFrame:
    """Extracts, processes, and verifies references."""
    # Create containers in the main area
    progress_text = st.empty()
    placeholder = st.empty()
    progress_text.text("æ­£åœ¨æå–å‚è€ƒæ–‡çŒ®...")

    try:
        references = split_references(bib_text)
    except ValueError as e:
        st.error(str(e))
        return pd.DataFrame()

    ref_type_dict = {"journal_article": "æœŸåˆŠæ–‡ç« ", "preprint": "é¢„å°æœ¬", "conference_paper": "ä¼šè®®è®ºæ–‡",
                     "book": "ä¹¦ç±", "book_chapter": "ä¹¦ç±ç« èŠ‚", "non_academic_website": "ç½‘ç«™"}
    status_emoji = {
        "validated": "âœ…å·²éªŒè¯",
        "invalid": "âŒæ— æ•ˆ",
        "not_found": "âš ï¸æœªæ‰¾åˆ°",
        "Pending": "â³å¤„ç†ä¸­"
    }

    results = []
    for idx, ref in enumerate(references):
        results.append({
            "Index": idx,
            "ç¬¬ä¸€ä½œè€…": ref.author,
            "å¹´ä»½": str(ref.year),
            "æ ‡é¢˜": ref.title,
            "ç±»å‹": ref_type_dict.get(ref.type, ref.type),
            "DOI": ref.DOI,
            "URL": ref.URL,
            "åŸå§‹æ–‡æœ¬": ref.bib,
            "çŠ¶æ€": "å¤„ç†ä¸­",
            "è¯´æ˜": "å¤„ç†ä¸­"
        })

    df = pd.DataFrame(results)

    # if URL is empty, and DOI is not empty: if DOI start wih https://, fill url with doi. Else, fill url with doi.org link
    df['URL'] = df.apply(
        lambda x: x['DOI'] if pd.notna(x['DOI']) and x['DOI'] != '' and (pd.isna(x['URL']) or x['URL'] == '') and x[
            'DOI'].startswith('https://') else f'https://doi.org/{x["DOI"]}' if pd.notna(x['DOI']) and x[
            'DOI'] != '' and (pd.isna(x['URL']) or x['URL'] == '') else x['URL'], axis=1)

    column_config = {
        "ç¬¬ä¸€ä½œè€…": st.column_config.TextColumn(
            help="ç¬¬ä¸€ä½œè€…çš„å§“æ°æˆ–æœºæ„"),
        "å¹´ä»½": st.column_config.TextColumn(width="small"),
        "URL": st.column_config.LinkColumn(width="medium"),
        "åŸå§‹æ–‡æœ¬": st.column_config.TextColumn(
            "åŸå§‹å‚è€ƒæ–‡çŒ®æ–‡æœ¬",  # Display name
            help="æ‚¬åœæŸ¥çœ‹å®Œæ•´æ–‡æœ¬",  # Tooltip message
            width="medium",  # Width of the column
        ),
        "çŠ¶æ€": st.column_config.TextColumn(
            help="å‚è€ƒæ–‡çŒ®éªŒè¯çŠ¶æ€"
        ),
        "è¯´æ˜": st.column_config.TextColumn(
            help="éªŒè¯ç»“æœçš„è¯´æ˜"
        )
    }

    df_display = df[[
        'ç¬¬ä¸€ä½œè€…', 'å¹´ä»½', 'æ ‡é¢˜', 'ç±»å‹', 'URL', 'åŸå§‹æ–‡æœ¬', 'çŠ¶æ€', 'è¯´æ˜']]
    placeholder.dataframe(df_display, use_container_width=True, column_config=column_config)

    verified_count = 0
    warning_count = 0
    progress_text.text(f"å·²éªŒè¯: {verified_count} | æ— æ•ˆ/æœªæ‰¾åˆ°: {warning_count}")

    for index, row in df.iterrows():
        result = search_title(references[index])
        df.loc[index, "çŠ¶æ€"] = status_emoji.get(result.status.value, result.status.value)
        df.loc[index, "è¯´æ˜"] = result.explanation
        if result.status == ReferenceStatus.VALIDATED:
            verified_count += 1
        else:
            warning_count += 1
        df_display = df[[
            'ç¬¬ä¸€ä½œè€…', 'å¹´ä»½', 'æ ‡é¢˜', 'ç±»å‹', 'URL', 'åŸå§‹æ–‡æœ¬', 'çŠ¶æ€', 'è¯´æ˜']]
        placeholder.dataframe(df_display, use_container_width=True, column_config=column_config)
        progress_text.text(f"å·²éªŒè¯: {verified_count} | æ— æ•ˆ/æœªæ‰¾åˆ°: {warning_count}")

    return df


def main():
    st.set_page_config(page_title="å­¦ä½‘æ˜Ÿé€”è®ºæ–‡å¼•ç”¨æ£€æµ‹å·¥å…·", page_icon="ğŸ”", layout="wide", initial_sidebar_state="expanded",
                       menu_items={
                           "About": "è¿™æ˜¯ä¸€ä¸ªç”¨äºéªŒè¯å­¦æœ¯è®ºæ–‡ä¸­å¼•ç”¨æ–‡çŒ®çš„å·¥å…·ã€‚åœ¨ [GitHub](https://github.com/ykangw/VeriExCiting) æŸ¥çœ‹æºä»£ç ã€‚"})

    st.title("å­¦ä½‘æ˜Ÿé€”è®ºæ–‡å¼•ç”¨æ£€æµ‹å·¥å…·")
    st.write(
        "æ­¤å·¥å…·å¸®åŠ©éªŒè¯å­¦æœ¯è®ºæ–‡ï¼ˆPDFæ ¼å¼ï¼‰ä¸­å¼•ç”¨æ–‡çŒ®çš„å­˜åœ¨æ€§ã€‚ "
        "å®ƒæå–å‚è€ƒæ–‡çŒ®ï¼Œè§£æå¼•ç”¨ï¼Œå¹¶æ£€æŸ¥å…¶æœ‰æ•ˆæ€§ã€‚"
    )

    with st.sidebar:
        st.header("è¾“å…¥")
        pdf_files = st.file_uploader("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªPDFæ–‡ä»¶", type="pdf", accept_multiple_files=True)

        st.write(
            "æ‚¨å¯ä»¥åœ¨ [Google AI Studio](https://ai.google.dev/aistudio) ç”³è¯· Gemini API å¯†é’¥ï¼Œæ¯å¤©å…è´¹æä¾›1500æ¬¡è¯·æ±‚ã€‚")
        api_key = st.text_input("è¾“å…¥æ‚¨çš„Google Gemini APIå¯†é’¥:", type="password")

    if st.sidebar.button("å¼€å§‹éªŒè¯"):
        if not pdf_files:
            st.warning("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªPDFæ–‡ä»¶ã€‚")
            return

        if not api_key:
            st.warning("è¯·è¾“å…¥Google Gemini APIå¯†é’¥ã€‚")
            return

        try:
            set_google_api_key(api_key)
            all_results = []

            for pdf_file in pdf_files:
                subheader = st.subheader(f"æ­£åœ¨å¤„ç†: {pdf_file.name}")
                bib_text = extract_bibliography_section(extract_text_from_pdf(pdf_file))

                # Display extracted bibliography text with expander
                with st.expander(f"{pdf_file.name} çš„æå–å‚è€ƒæ–‡çŒ®æ–‡æœ¬"):
                    st.text_area("æå–çš„æ–‡æœ¬", bib_text, height=200, label_visibility="hidden")

                results_df = process_and_verify(bib_text)
                results_df['æºæ–‡ä»¶'] = pdf_file.name
                all_results.append(results_df)
                subheader.subheader(f"å·²å®Œæˆ: {pdf_file.name}")

            if all_results:
                combined_results = pd.concat(all_results, ignore_index=True)
                csv = combined_results.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="ä¸‹è½½æ‰€æœ‰ç»“æœä¸ºCSV",
                    data=csv,
                    file_name='å­¦ä½‘æ˜Ÿé€”å¼•ç”¨æ£€æµ‹ç»“æœ.csv',
                    mime='text/csv',
                )

        except ValueError as ve:
            st.error(str(ve))
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()