import streamlit as st
from veriexcite import (
    extract_bibliography_section,
    split_references,
    search_title,
    set_google_api_key,
    ReferenceStatus,
)
import io
import pandas as pd
import PyPDF2


def extract_text_from_pdf(pdf_file: st.runtime.uploaded_file_manager.UploadedFile) -> str:
    """æ ¡éªŒæ˜¯å¦ä¸º PDFï¼Œå¹¶æŠ½å–å…¨æ–‡æ–‡æœ¬ã€‚"""
    if not pdf_file.name.lower().endswith(".pdf"):
        raise ValueError("ä¸Šä¼ çš„æ–‡ä»¶ä¸æ˜¯ PDFã€‚")
    pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file.read()))
    pdf_content = ""
    for page in pdf_reader.pages:
        page_text = page.extract_text()
        if page_text:
            pdf_content += page_text + "\n"
    return pdf_content


def process_and_verify(bib_text: str, keywords=["å‚è€ƒæ–‡çŒ®", "References", "Bibliography", "Works Cited"]) -> pd.DataFrame:
    """æŠ½å–ã€è§£æå¹¶æ ¡éªŒå‚è€ƒæ–‡çŒ®ã€‚"""
    # è¿›åº¦ä¸å ä½å®¹å™¨
    progress_text = st.empty()
    placeholder = st.empty()
    progress_text.text("æ­£åœ¨æå–å‚è€ƒæ–‡çŒ®...")

    try:
        references = split_references(bib_text)
    except ValueError as e:
        st.error(str(e))
        return pd.DataFrame()

    ref_type_dict = {
        "journal_article": "æœŸåˆŠè®ºæ–‡",
        "preprint": "é¢„å°æœ¬",
        "conference_paper": "ä¼šè®®è®ºæ–‡",
        "book": "ä¹¦ç±",
        "book_chapter": "ä¹¦ç±ç« èŠ‚",
        "non_academic_website": "ç½‘ç«™",
    }
    status_emoji = {
        "validated": "âœ… å·²éªŒè¯",
        "invalid": "âŒ æ— æ•ˆ",
        "not_found": "âš ï¸ æœªæ‰¾åˆ°",
        "Pending": "â³ å¾…éªŒè¯",
    }

    results = []
    for idx, ref in enumerate(references):
        results.append({
            "åºå·": idx,
            "ç¬¬ä¸€ä½œè€…": ref.author,
            "å¹´ä»½": str(ref.year),
            "æ ‡é¢˜": ref.title,
            "ç±»å‹": ref_type_dict.get(ref.type, ref.type),
            "DOI": ref.DOI,
            "é“¾æ¥": ref.URL,
            "åŸå§‹æ–‡æœ¬": ref.bib,
            "çŠ¶æ€": "å¾…éªŒè¯",
            "è¯´æ˜": "å¾…éªŒè¯"
        })

    df = pd.DataFrame(results)

    # è‹¥ URL ä¸ºç©ºä½† DOI å­˜åœ¨ï¼šè‹¥ DOI ä»¥ https:// å¼€å¤´ï¼Œç›´æ¥ç”¨ä½œé“¾æ¥ï¼›å¦åˆ™è¡¥å…¨ä¸º https://doi.org/<DOI>
    df['é“¾æ¥'] = df.apply(
        lambda x: x['DOI'] if pd.notna(x['DOI']) and x['DOI'] != '' and (pd.isna(x['é“¾æ¥']) or x['é“¾æ¥'] == '') and x['DOI'].startswith('https://')
        else (f'https://doi.org/{x["DOI"]}' if pd.notna(x['DOI']) and x['DOI'] != '' and (pd.isna(x['é“¾æ¥']) or x['é“¾æ¥'] == '') else x['é“¾æ¥']),
        axis=1
    )

    column_config = {
        "ç¬¬ä¸€ä½œè€…": st.column_config.TextColumn(help="ç¬¬ä¸€ä½œè€…çš„å§“æ°ï¼Œæˆ–æœºæ„åç§°"),
        "å¹´ä»½": st.column_config.TextColumn(width="small"),
        "é“¾æ¥": st.column_config.LinkColumn(width="medium"),
        "åŸå§‹æ–‡æœ¬": st.column_config.TextColumn(
            "åŸå§‹å‚è€ƒæ–‡çŒ®",
            help="æ‚¬åœå¯æŸ¥çœ‹å®Œæ•´æ–‡æœ¬",
            width="medium",
        ),
        "çŠ¶æ€": st.column_config.TextColumn(help="å‚è€ƒæ–‡çŒ®æ ¡éªŒçŠ¶æ€"),
        "è¯´æ˜": st.column_config.TextColumn(help="ç»“æœè¯´æ˜"),
    }

    df_display = df[['ç¬¬ä¸€ä½œè€…', 'å¹´ä»½', 'æ ‡é¢˜', 'ç±»å‹', 'é“¾æ¥', 'åŸå§‹æ–‡æœ¬', 'çŠ¶æ€', 'è¯´æ˜']]
    placeholder.dataframe(df_display, use_container_width=True, column_config=column_config)

    verified_count = 0
    warning_count = 0
    progress_text.text(f"å·²éªŒè¯ï¼š{verified_count} | å¼‚å¸¸/æœªæ‰¾åˆ°ï¼š{warning_count}")

    for index, row in df.iterrows():
        result = search_title(references[index])
        df.loc[index, "çŠ¶æ€"] = status_emoji.get(result.status.value, result.status.value)
        df.loc[index, "è¯´æ˜"] = result.explanation
        if result.status == ReferenceStatus.VALIDATED:
            verified_count += 1
        else:
            warning_count += 1
        df_display = df[['ç¬¬ä¸€ä½œè€…', 'å¹´ä»½', 'æ ‡é¢˜', 'ç±»å‹', 'é“¾æ¥', 'åŸå§‹æ–‡æœ¬', 'çŠ¶æ€', 'è¯´æ˜']]
        placeholder.dataframe(df_display, use_container_width=True, column_config=column_config)
        progress_text.text(f"å·²éªŒè¯ï¼š{verified_count} | å¼‚å¸¸/æœªæ‰¾åˆ°ï¼š{warning_count}")

    return df


def main():
    st.set_page_config(
        page_title="å­¦ä½‘æ˜Ÿé€”è®ºæ–‡æ£€æŸ¥",
        page_icon="ğŸ”",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    st.title("å­¦ä½‘æ˜Ÿé€”è®ºæ–‡æ£€æŸ¥")
    st.write(
        "æœ¬å·¥å…·ç”¨äºå¯¹å­¦æœ¯è®ºæ–‡ï¼ˆPDFï¼‰ä¸­çš„å‚è€ƒæ–‡çŒ®è¿›è¡Œæ ¸éªŒï¼šè‡ªåŠ¨æå–â€œå‚è€ƒæ–‡çŒ®â€æ¿å—ï¼Œè§£æå„æ¡ç›®ï¼Œå¹¶é€šè¿‡æ£€ç´¢éªŒè¯å…¶å­˜åœ¨æ€§ä¸å¯ä¿¡åº¦ã€‚"
    )

    with st.sidebar:
        st.header("è¾“å…¥")
        pdf_files = st.file_uploader("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª PDF æ–‡ä»¶", type="pdf", accept_multiple_files=True)

        st.write("è¦ä½¿ç”¨åœ¨çº¿æ£€ç´¢ï¼Œéœ€è¦æä¾› Google Gemini çš„ API Keyï¼ˆä¸ªäººå¯†é’¥ï¼‰ã€‚")
        st.markdown(
            "å‰å¾€ [Google AI Studio](https://ai.google.dev/aistudio) ç”³è¯·ï¼ˆç›®å‰å¸¸è§é…é¢ä¸ºæ¯æ—¥å…è´¹ä¸€å®šæ¬¡æ•°ï¼‰ã€‚"
        )
        api_key = st.text_input("è¯·è¾“å…¥ä½ çš„ Google Gemini API Keyï¼š", type="password", help="å¯†é’¥ä»…ç”¨äºæœ¬åœ°æ£€ç´¢ä¸æ ¡éªŒï¼Œä¸ä¼šä¸Šä¼ è‡³æœåŠ¡å™¨ã€‚")

    if st.sidebar.button("å¼€å§‹æ ¡éªŒ"):
        if not pdf_files:
            st.warning("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ª PDF æ–‡ä»¶ã€‚")
            return

        if not api_key:
            st.warning("è¯·è¾“å…¥ Google Gemini API Keyã€‚")
            return

        try:
            set_google_api_key(api_key)
            all_results = []

            for pdf_file in pdf_files:
                subheader = st.subheader(f"å¤„ç†ä¸­ï¼š{pdf_file.name}")
                bib_text = extract_bibliography_section(extract_text_from_pdf(pdf_file))

                # å±•ç¤ºæŠ½å–åˆ°çš„å‚è€ƒæ–‡çŒ®æ–‡æœ¬
                with st.expander(f"å‚è€ƒæ–‡çŒ®æŠ½å–ç»“æœï¼š{pdf_file.name}"):
                    st.text_area("æŠ½å–æ–‡æœ¬", bib_text, height=200, label_visibility="hidden")

                results_df = process_and_verify(bib_text)
                results_df['æ¥æºæ–‡ä»¶'] = pdf_file.name
                all_results.append(results_df)
                subheader.subheader(f"å®Œæˆï¼š{pdf_file.name}")

            if all_results:
                combined_results = pd.concat(all_results, ignore_index=True)
                csv = combined_results.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="ä¸‹è½½æ‰€æœ‰ç»“æœï¼ˆCSVï¼‰",
                    data=csv,
                    file_name='å­¦ä½‘æ˜Ÿé€”_å¼•ç”¨æ ¡éªŒç»“æœ.csv',
                    mime='text/csv',
                )

        except ValueError as ve:
            st.error(str(ve))
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")


if __name__ == "__main__":
    main()
